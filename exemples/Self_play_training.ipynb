{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasBertola/Connect-4-Gym-env-Reinforcement-learning/blob/main/exemples/Self_play_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing dependencies"
      ],
      "metadata": {
        "id": "CLLYEmPH6z4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gymnasium-connect-four==1.3.1 stable-baselines3==2.0.0"
      ],
      "metadata": {
        "id": "aM8D-1Wh66xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self training"
      ],
      "metadata": {
        "id": "PCgQRcpb7Y0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from connect_four_gymnasium import ConnectFourEnv\n",
        "from connect_four_gymnasium.players import ModelPlayer\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "env = ConnectFourEnv()\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "opponent =  ModelPlayer(model,name=\"yourself\")\n",
        "env.change_opponent(opponent)"
      ],
      "metadata": {
        "id": "52SZhLy67csz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_3k9zAWBPL5"
      },
      "outputs": [],
      "source": [
        "model.learn(total_timesteps=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test your model"
      ],
      "metadata": {
        "id": "094XwzvH8j1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from connect_four_gymnasium.tools import EloLeaderboard\n",
        "from connect_four_gymnasium.players import ModelPlayer\n",
        "\n",
        "print('Your elo: ',EloLeaderboard().get_elo(opponent, num_matches=250))"
      ],
      "metadata": {
        "id": "4hlWWyN58pKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus : Learning from the games of both opponents"
      ],
      "metadata": {
        "id": "lcO8AF14jBeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may have noticed, we have just played against ourselves to learn, but we are greatly underutilizing the playing time. Indeed, we learn from player 1, but not from player 2 (despite it being the same model as player 1).\n",
        "\n",
        "In order to double the learning base without spending more time on simulation, it can be useful to learn from the perspective of both opponents. Here's how to retrieve the games. (Please note that PPO does not allow learning from games provided as input, so you will need to use another library (or your own) than Stable Baselines to take advantage of these games)."
      ],
      "metadata": {
        "id": "f3SpgG9GkVjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "env = ConnectFourEnv() #without opponent\n",
        "AllGamesMoves = []\n",
        "GameToPlay = 100\n",
        "for i in range(GameToPlay):\n",
        "  gameIsFinish = False\n",
        "  moves = []\n",
        "  obs , _=  env.reset()\n",
        "  while gameIsFinish == False:\n",
        "    action, _state = model.predict(obs)\n",
        "    moves.append((np.copy(obs),int(action), 0)) # we will update 0 later, it's the final score for this player\n",
        "    obs, rewards, dones, truncated, info = env.step(action)\n",
        "    if truncated or dones:\n",
        "      # Update rewards for each move by iterating through the reversed moves list\n",
        "      # If the index of the move is even or odd like the last move's index, assign the positive reward, otherwise assign the negative reward\n",
        "      moves = [(move[0], move[1], rewards if idx % 2 == (len(moves) - 1) % 2 else -rewards) for idx, move in enumerate(reversed(moves))]\n",
        "      gameIsFinish=True\n",
        "      AllGamesMoves.extend(moves)\n",
        "\n",
        "print(len(AllGamesMoves))\n"
      ],
      "metadata": {
        "id": "30NSH9aAkbq0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}